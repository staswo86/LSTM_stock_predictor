{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "baxAlMuODzzu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def read_data_function(file_name):\n",
        "    data_frame = pd.read_csv(file_name, sep = \",\")\n",
        "    data_frame = data_frame[[\"date\", \"open\"]]\n",
        "    data_frame[\"date\"] = pd.to_datetime(data_frame[\"date\"], format = \"%Y-%m-%d\")\n",
        "    data_frame.index = data_frame.pop(\"date\")\n",
        "    #scaler = MinMaxScaler(feature_range = (-1, 1))\n",
        "    #stock_price_val = data_frame[\"open\"].values.reshape(-1,1)\n",
        "    #data_frame[\"open\"] = scaler.fit_transform(stock_price_val)\n",
        "\n",
        "\n",
        "    return data_frame"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "def data_prep_function(sequence_dataset,window_size):\n",
        "    #sequence_dataset = [np.array(sequence_dataset[i*sliding_window_size: (i + 1) * sliding_window_size]) for i in range(len(sequence_dataset) // sliding_window_size)]\n",
        "    #X = np.array([sequenc_dataset[i: i + num_steps] for i in range(len(sequence_dataset) - sliding_window_size)])\n",
        "    #y = np.array([sequence_dataset[i + num_steps] for i in range(len(sequence_dataset) - sliding_window_size)])\n",
        "    for i in range(1, window_size + 1):\n",
        "        sequence_dataset[f\"open-{i}\"] = sequence_dataset[\"open\"].shift(i)\n",
        "\n",
        "\n",
        "    sequence_dataset.dropna(inplace = True)\n",
        "\n",
        "\n",
        "    return sequence_dataset"
      ],
      "metadata": {
        "id": "EN7bdkTsEJKq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "def matrix_formatting_and_normalize_dataset(sliding_window_dataset):\n",
        "    sliding_window_dataset = sliding_window_dataset.to_numpy()\n",
        "    scaler = MinMaxScaler()\n",
        "    normalized_data = scaler.fit_transform(sliding_window_dataset)\n",
        "    return normalized_data"
      ],
      "metadata": {
        "id": "xUi3qzNmEOF7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import torch\n",
        "def feature_target_modeling(dataset):\n",
        "    X = dataset[:, 1:]\n",
        "    y = dataset[:,0]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1)\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "    y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
        "    y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
        "    X_train_tensor = torch.tensor(X_train, dtype = torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype = torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype = torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype = torch.float32)\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    train_size = int(0.8 * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    train_sub, val_sub = random_split(train_dataset, [train_size, val_size])\n",
        "    val_loader = torch.utils.data.DataLoader(val_sub, batch_size=16, shuffle=False)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 16, shuffle = True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 16, shuffle = False)\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "b-PdYgcqGDdK"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "class CustomLSTM(nn.Module):\n",
        "    def __init__(self, input_sz, hidden_sz):\n",
        "        super().__init__()\n",
        "        self.input_sz= input_sz\n",
        "        self.hidden_size= hidden_sz\n",
        "        self.W= nn.Parameter(torch.Tensor(input_sz, hidden_sz* 4))\n",
        "        self.U= nn.Parameter(torch.Tensor(hidden_sz, hidden_sz* 4))\n",
        "        self.bias= nn.Parameter(torch.Tensor(hidden_sz* 4))\n",
        "        self.linear = nn.Linear(input_sz, hidden_sz)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv= 1.0/ math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x,\n",
        "                init_states=None):\n",
        "\n",
        "\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        bs, seq_sz, _= x.size()\n",
        "        hidden_seq= []\n",
        "        if init_states is None:\n",
        "\n",
        "            h_t, c_t= (torch.zeros(bs, self.hidden_size).to(x.device),\n",
        "                            torch.zeros(bs, self.hidden_size).to(x.device))\n",
        "        else:\n",
        "\n",
        "             h_t, c_t= init_states\n",
        "\n",
        "        HS= self.hidden_size\n",
        "        for t in range(seq_sz):\n",
        "                        x_t= x[:, t, :]\n",
        "            # batch the computations into a single matrix multiplication\n",
        "                        gates= x_t@ self.W+ h_t@ self.U+ self.bias\n",
        "                        i_t, f_t, g_t, o_t= (\n",
        "                            torch.sigmoid(gates[:, :HS]),# input\n",
        "                            torch.sigmoid(gates[:, HS:HS*2]),# forget\n",
        "                            torch.tanh(gates[:, HS*2:HS*3]),\n",
        "                            torch.sigmoid(gates[:, HS*3:]),# output\n",
        "                        )\n",
        "                        c_t= f_t* c_t+ i_t* g_t\n",
        "                        h_t= o_t* torch.tanh(c_t)\n",
        "                        hidden_seq.append(h_t.unsqueeze(0))\n",
        "        hidden_seq= torch.cat(hidden_seq, dim=0)\n",
        "            # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
        "        hidden_seq= hidden_seq.transpose(0, 1).contiguous()\n",
        "        output = self.linear(hidden_seq[:, -1, :])\n",
        "        return output"
      ],
      "metadata": {
        "id": "tP3T7yv7GKJn"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def train_model(model: torch.nn.Module, train_dataloader : DataLoader,val_dataloader: DataLoader, epochs: int):\n",
        "    model.train(True)\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    loss_function = torch.nn.MSELoss()\n",
        "    loss_vectorize_train = np.zeros(epochs)\n",
        "    loss_vectorize_val = np.zeros(epochs)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss_train = 0\n",
        "        for x_batch, y_batch in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(x_batch)\n",
        "            loss = loss_function(preds, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss_train += loss.item()\n",
        "            loss_vectorize_train[epoch] = epoch_loss_train / len(train_dataloader)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            epoch_loss_val = 0\n",
        "            for x_batch, y_batch in val_dataloader:\n",
        "                preds_2 = model(x_batch)\n",
        "                loss = loss_function(preds_2, y_batch)\n",
        "                epoch_loss_val += loss.item()\n",
        "                loss_vectorize_val[epoch] = epoch_loss_val / len(val_dataloader)\n",
        "\n",
        "        print(f\"Epoch {epoch:.3d}: Train loss {loss_vectorize_train[epoch]} || Val loss {loss_vectorize_val[epoch]}\")\n",
        "\n",
        "\n",
        "\n",
        "    return loss_vectorize_train, loss_vectorize_val"
      ],
      "metadata": {
        "id": "VI9VCxH2GOdz"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_data_function(\"/all_stocks_5yr.csv\")"
      ],
      "metadata": {
        "id": "nkGp-kBWGSld"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_data = data_prep_function(data, window_size = 10)"
      ],
      "metadata": {
        "id": "LOdHkg1xIdOV"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formated_data = matrix_formatting_and_normalize_dataset(window_data)"
      ],
      "metadata": {
        "id": "Mnovcug4Idv6"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader, test_loader = feature_target_modeling(formated_data)"
      ],
      "metadata": {
        "id": "He3sHR_8Id7Y"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomLSTM(1, 6)"
      ],
      "metadata": {
        "id": "Ae7_rZj5IeFh"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, val_loader,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "vlDlL5_LIeOp",
        "outputId": "0e9aecd4-96b7-4b6f-a427-22e9d89c413d"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (16x6 and 1x6)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2432148487.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1528777056.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4294845013.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, init_states)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mhidden_seq\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mhidden_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x6 and 1x6)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aNJzOzXdIeXO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}