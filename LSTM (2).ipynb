{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "baxAlMuODzzu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def read_data_function(file_name):\n",
        "    data_frame = pd.read_csv(file_name, sep = \",\")\n",
        "    data_frame = data_frame[[\"date\", \"open\"]]\n",
        "    data_frame[\"date\"] = pd.to_datetime(data_frame[\"date\"], format = \"%Y-%m-%d\")\n",
        "    data_frame.index = data_frame.pop(\"date\")\n",
        "    #scaler = MinMaxScaler(feature_range = (-1, 1))\n",
        "    #stock_price_val = data_frame[\"open\"].values.reshape(-1,1)\n",
        "    #data_frame[\"open\"] = scaler.fit_transform(stock_price_val)\n",
        "\n",
        "\n",
        "    return data_frame"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "def data_prep_function(sequence_dataset,window_size):\n",
        "    #sequence_dataset = [np.array(sequence_dataset[i*sliding_window_size: (i + 1) * sliding_window_size]) for i in range(len(sequence_dataset) // sliding_window_size)]\n",
        "    #X = np.array([sequenc_dataset[i: i + num_steps] for i in range(len(sequence_dataset) - sliding_window_size)])\n",
        "    #y = np.array([sequence_dataset[i + num_steps] for i in range(len(sequence_dataset) - sliding_window_size)])\n",
        "    for i in range(1, window_size + 1):\n",
        "        sequence_dataset[f\"open-{i}\"] = sequence_dataset[\"open\"].shift(i)\n",
        "\n",
        "\n",
        "    sequence_dataset.dropna(inplace = True)\n",
        "\n",
        "\n",
        "    return sequence_dataset"
      ],
      "metadata": {
        "id": "EN7bdkTsEJKq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "def matrix_formatting_and_normalize_dataset(sliding_window_dataset):\n",
        "    sliding_window_dataset = sliding_window_dataset.to_numpy()\n",
        "    scaler = MinMaxScaler()\n",
        "    normalized_data = scaler.fit_transform(sliding_window_dataset)\n",
        "    return normalized_data"
      ],
      "metadata": {
        "id": "xUi3qzNmEOF7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import torch\n",
        "def feature_target_modeling(dataset):\n",
        "    X = dataset[:, 1:]\n",
        "    y = dataset[:,0]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1)\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "    y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
        "    y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
        "    X_train_tensor = torch.tensor(X_train, dtype = torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype = torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype = torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype = torch.float32)\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    train_size = int(0.8 * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    train_sub, val_sub = random_split(train_dataset, [train_size, val_size])\n",
        "    val_loader = torch.utils.data.DataLoader(val_sub, batch_size=16, shuffle=False)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 16, shuffle = True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 16, shuffle = False)\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "b-PdYgcqGDdK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "class CustomLSTM(nn.Module):\n",
        "    def __init__(self, input_sz, hidden_sz, output_sz = 1):\n",
        "        super().__init__()\n",
        "        self.input_sz= input_sz\n",
        "        self.hidden_size= hidden_sz\n",
        "        self.W= nn.Parameter(torch.Tensor(input_sz, hidden_sz* 4))\n",
        "        self.U= nn.Parameter(torch.Tensor(hidden_sz, hidden_sz* 4))\n",
        "        self.bias= nn.Parameter(torch.Tensor(hidden_sz* 4))\n",
        "        self.linear = nn.Linear(hidden_sz, output_sz)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv= 1.0/ math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x,\n",
        "                init_states=None):\n",
        "\n",
        "\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        bs, seq_sz, _= x.size()\n",
        "        hidden_seq= []\n",
        "        if init_states is None:\n",
        "\n",
        "            h_t, c_t= (torch.zeros(bs, self.hidden_size).to(x.device),\n",
        "                            torch.zeros(bs, self.hidden_size).to(x.device))\n",
        "        else:\n",
        "\n",
        "             h_t, c_t= init_states\n",
        "\n",
        "        HS= self.hidden_size\n",
        "        for t in range(seq_sz):\n",
        "                        x_t= x[:, t, :]\n",
        "            # batch the computations into a single matrix multiplication\n",
        "                        gates= x_t@ self.W+ h_t@ self.U+ self.bias\n",
        "                        i_t, f_t, g_t, o_t= (\n",
        "                            torch.sigmoid(gates[:, :HS]),# input\n",
        "                            torch.sigmoid(gates[:, HS:HS*2]),# forget\n",
        "                            torch.tanh(gates[:, HS*2:HS*3]),\n",
        "                            torch.sigmoid(gates[:, HS*3:]),# output\n",
        "                        )\n",
        "                        c_t= f_t* c_t+ i_t* g_t\n",
        "                        h_t= o_t* torch.tanh(c_t)\n",
        "                        hidden_seq.append(h_t.unsqueeze(0))\n",
        "        hidden_seq= torch.cat(hidden_seq, dim=0)\n",
        "            # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
        "        hidden_seq= hidden_seq.transpose(0, 1).contiguous()\n",
        "        output = self.linear(hidden_seq[:, -1, :])\n",
        "        return output"
      ],
      "metadata": {
        "id": "tP3T7yv7GKJn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def train_model(model: torch.nn.Module, train_dataloader : DataLoader,val_dataloader: DataLoader, epochs: int):\n",
        "    model.train(True)\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    loss_function = torch.nn.MSELoss()\n",
        "    loss_vectorize_train = np.zeros(epochs)\n",
        "    loss_vectorize_val = np.zeros(epochs)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss_train = 0\n",
        "        for x_batch, y_batch in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(x_batch)\n",
        "            loss = loss_function(preds, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss_train += loss.item()\n",
        "            loss_vectorize_train[epoch] = epoch_loss_train / len(train_dataloader)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            epoch_loss_val = 0\n",
        "            for x_batch, y_batch in val_dataloader:\n",
        "                preds_2 = model(x_batch)\n",
        "                loss = loss_function(preds_2, y_batch)\n",
        "                epoch_loss_val += loss.item()\n",
        "                loss_vectorize_val[epoch] = epoch_loss_val / len(val_dataloader)\n",
        "\n",
        "        print(f\"Epoch {epoch}: Train loss {loss_vectorize_train[epoch]} || Val loss {loss_vectorize_val[epoch]}\")\n",
        "\n",
        "\n",
        "\n",
        "    return loss_vectorize_train, loss_vectorize_val"
      ],
      "metadata": {
        "id": "VI9VCxH2GOdz"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_data_function(\"/content/all_stocks_5yr.csv\")"
      ],
      "metadata": {
        "id": "nkGp-kBWGSld"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_data = data_prep_function(data, window_size = 10)"
      ],
      "metadata": {
        "id": "LOdHkg1xIdOV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formated_data = matrix_formatting_and_normalize_dataset(window_data)"
      ],
      "metadata": {
        "id": "Mnovcug4Idv6"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader, test_loader = feature_target_modeling(formated_data)"
      ],
      "metadata": {
        "id": "He3sHR_8Id7Y"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomLSTM(1, 6)"
      ],
      "metadata": {
        "id": "Ae7_rZj5IeFh"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, val_loader,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlDlL5_LIeOp",
        "outputId": "e6da1254-fd2b-4a72-97c3-1b36d8a8b0d1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train loss 8.901507526637503e-05 || Val loss 1.3462626371119658e-05\n",
            "Epoch 1: Train loss 2.0444945395780862e-05 || Val loss 8.018274645387992e-06\n",
            "Epoch 2: Train loss 1.5493497132401118e-05 || Val loss 1.5468163526670698e-05\n",
            "Epoch 3: Train loss 1.341125785662444e-05 || Val loss 7.423701593574071e-06\n",
            "Epoch 4: Train loss 1.310169851021806e-05 || Val loss 5.792445725423622e-06\n",
            "Epoch 5: Train loss 1.1507120935756445e-05 || Val loss 8.245576470949792e-06\n",
            "Epoch 6: Train loss 1.1183485701574818e-05 || Val loss 5.590535873645345e-06\n",
            "Epoch 7: Train loss 1.0418459178574572e-05 || Val loss 6.736272205645746e-06\n",
            "Epoch 8: Train loss 9.788952630761557e-06 || Val loss 5.400209990628549e-06\n",
            "Epoch 9: Train loss 9.299756278356487e-06 || Val loss 7.3180170786737575e-06\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([8.90150753e-05, 2.04449454e-05, 1.54934971e-05, 1.34112579e-05,\n",
              "        1.31016985e-05, 1.15071209e-05, 1.11834857e-05, 1.04184592e-05,\n",
              "        9.78895263e-06, 9.29975628e-06]),\n",
              " array([1.34626264e-05, 8.01827465e-06, 1.54681635e-05, 7.42370159e-06,\n",
              "        5.79244573e-06, 8.24557647e-06, 5.59053587e-06, 6.73627221e-06,\n",
              "        5.40020999e-06, 7.31801708e-06]))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aNJzOzXdIeXO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}