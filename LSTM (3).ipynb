{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "baxAlMuODzzu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def read_data_function(file_name):\n",
        "    data_frame = pd.read_csv(file_name, sep = \",\")\n",
        "    data_frame = data_frame[[\"date\", \"open\"]]\n",
        "    data_frame[\"date\"] = pd.to_datetime(data_frame[\"date\"], format = \"%Y-%m-%d\")\n",
        "    data_frame.index = data_frame.pop(\"date\")\n",
        "    #scaler = MinMaxScaler(feature_range = (-1, 1))\n",
        "    #stock_price_val = data_frame[\"open\"].values.reshape(-1,1)\n",
        "    #data_frame[\"open\"] = scaler.fit_transform(stock_price_val)\n",
        "\n",
        "\n",
        "    return data_frame"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "def data_prep_function(sequence_dataset,window_size):\n",
        "    #sequence_dataset = [np.array(sequence_dataset[i*sliding_window_size: (i + 1) * sliding_window_size]) for i in range(len(sequence_dataset) // sliding_window_size)]\n",
        "    #X = np.array([sequenc_dataset[i: i + num_steps] for i in range(len(sequence_dataset) - sliding_window_size)])\n",
        "    #y = np.array([sequence_dataset[i + num_steps] for i in range(len(sequence_dataset) - sliding_window_size)])\n",
        "    for i in range(1, window_size + 1):\n",
        "        sequence_dataset[f\"open-{i}\"] = sequence_dataset[\"open\"].shift(i)\n",
        "\n",
        "\n",
        "    sequence_dataset.dropna(inplace = True)\n",
        "\n",
        "\n",
        "    return sequence_dataset"
      ],
      "metadata": {
        "id": "EN7bdkTsEJKq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "def matrix_formatting_and_normalize_dataset(sliding_window_dataset):\n",
        "    sliding_window_dataset = sliding_window_dataset.to_numpy()\n",
        "    scaler = MinMaxScaler()\n",
        "    normalized_data = scaler.fit_transform(sliding_window_dataset)\n",
        "    return normalized_data"
      ],
      "metadata": {
        "id": "xUi3qzNmEOF7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import torch\n",
        "def feature_target_modeling(dataset):\n",
        "    X = dataset[:, 1:]\n",
        "    y = dataset[:,0]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1)\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "    y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
        "    y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
        "    X_train_tensor = torch.tensor(X_train, dtype = torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype = torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype = torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype = torch.float32)\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    train_size = int(0.8 * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    train_sub, val_sub = random_split(train_dataset, [train_size, val_size])\n",
        "    val_loader = torch.utils.data.DataLoader(val_sub, batch_size=16, shuffle=False)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 16, shuffle = True)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 16, shuffle = False)\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "b-PdYgcqGDdK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "class CustomLSTM(nn.Module):\n",
        "    def __init__(self, input_sz, hidden_sz, output_sz = 1):\n",
        "        super().__init__()\n",
        "        self.input_sz= input_sz\n",
        "        self.hidden_size= hidden_sz\n",
        "        self.W= nn.Parameter(torch.Tensor(input_sz, hidden_sz* 4))\n",
        "        self.U= nn.Parameter(torch.Tensor(hidden_sz, hidden_sz* 4))\n",
        "        self.bias= nn.Parameter(torch.Tensor(hidden_sz* 4))\n",
        "        self.linear = nn.Linear(hidden_sz, output_sz)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        stdv= 1.0/ math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x,\n",
        "                init_states=None):\n",
        "\n",
        "\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        bs, seq_sz, _= x.size()\n",
        "        hidden_seq= []\n",
        "        if init_states is None:\n",
        "\n",
        "            h_t, c_t= (torch.zeros(bs, self.hidden_size).to(x.device),\n",
        "                            torch.zeros(bs, self.hidden_size).to(x.device))\n",
        "        else:\n",
        "\n",
        "             h_t, c_t= init_states\n",
        "\n",
        "        HS= self.hidden_size\n",
        "        for t in range(seq_sz):\n",
        "                        x_t= x[:, t, :]\n",
        "            # batch the computations into a single matrix multiplication\n",
        "                        gates= x_t@ self.W+ h_t@ self.U+ self.bias\n",
        "                        i_t, f_t, g_t, o_t= (\n",
        "                            torch.sigmoid(gates[:, :HS]),# input\n",
        "                            torch.sigmoid(gates[:, HS:HS*2]),# forget\n",
        "                            torch.tanh(gates[:, HS*2:HS*3]),\n",
        "                            torch.sigmoid(gates[:, HS*3:]),# output\n",
        "                        )\n",
        "                        c_t= f_t* c_t+ i_t* g_t\n",
        "                        h_t= o_t* torch.tanh(c_t)\n",
        "                        hidden_seq.append(h_t.unsqueeze(0))\n",
        "        hidden_seq= torch.cat(hidden_seq, dim=0)\n",
        "            # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
        "        hidden_seq= hidden_seq.transpose(0, 1).contiguous()\n",
        "        output = self.linear(hidden_seq[:, -1, :])\n",
        "        return output"
      ],
      "metadata": {
        "id": "tP3T7yv7GKJn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def train_model(model: torch.nn.Module, train_dataloader : DataLoader,val_dataloader: DataLoader, epochs: int):\n",
        "    model.train(True)\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    loss_function = torch.nn.MSELoss()\n",
        "    loss_vectorize_train = np.zeros(epochs)\n",
        "    loss_vectorize_val = np.zeros(epochs)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss_train = 0\n",
        "        for x_batch, y_batch in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(x_batch)\n",
        "            loss = loss_function(preds, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss_train += loss.item()\n",
        "            loss_vectorize_train[epoch] = epoch_loss_train / len(train_dataloader)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            epoch_loss_val = 0\n",
        "            for x_batch_val, y_batch_val in val_dataloader:\n",
        "                preds_2 = model(x_batch_val)\n",
        "                loss = loss_function(preds_2, y_batch_val)\n",
        "                epoch_loss_val += loss.item()\n",
        "                loss_vectorize_val[epoch] = epoch_loss_val / len(val_dataloader)\n",
        "\n",
        "        print(f\"Epoch {epoch}: Train loss {loss_vectorize_train[epoch] : .6f} || Val loss {loss_vectorize_val[epoch] : .6f}\")\n",
        "\n",
        "\n",
        "\n",
        "    return loss_vectorize_train, loss_vectorize_val, y_batch[1]"
      ],
      "metadata": {
        "id": "VI9VCxH2GOdz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "def prediction_printer(test_loader : DataLoader, model):\n",
        "  with torch.no_grad():\n",
        "    for x_batch, y_batch in test_loader:\n",
        "      prediction_test = model(x_batch).numpy()\n",
        "\n",
        "  return prediction_test\n",
        "\n"
      ],
      "metadata": {
        "id": "ojDPc76RaaMA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_data_function(\"/content/all_stocks_5yr.csv\")"
      ],
      "metadata": {
        "id": "nkGp-kBWGSld",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "9d7328de-6d13-466d-ac23-405da85b83a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/all_stocks_5yr.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1190641686.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/all_stocks_5yr.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2674374572.py\u001b[0m in \u001b[0;36mread_data_function\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_data_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdata_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mdata_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"open\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdata_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%Y-%m-%d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/all_stocks_5yr.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_data_10 = data_prep_function(data, window_size = 10)"
      ],
      "metadata": {
        "id": "LOdHkg1xIdOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NyCrhWM0uSq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formated_data_10 = matrix_formatting_and_normalize_dataset(window_data_10)"
      ],
      "metadata": {
        "id": "Mnovcug4Idv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_10, val_loader_10, test_loader_10 = feature_target_modeling(formated_data_10)"
      ],
      "metadata": {
        "id": "He3sHR_8Id7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomLSTM(1, 6)"
      ],
      "metadata": {
        "id": "Ae7_rZj5IeFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_train_10, loss_val_10, y_train_10 = train_model(model, train_loader_10, val_loader_10,4)"
      ],
      "metadata": {
        "id": "SRlsP0OcNdO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_10 = prediction_printer(test_loader_10, model)"
      ],
      "metadata": {
        "id": "ntJ5rZ92J6OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_data_50 = data_prep_function(data, window_size = 50)"
      ],
      "metadata": {
        "id": "aNJzOzXdIeXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formated_data_50 = matrix_formatting_and_normalize_dataset(window_data_50)"
      ],
      "metadata": {
        "id": "mEM5voyIrRJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_50, val_loader_50, test_loader_50 = feature_target_modeling(formated_data_50)"
      ],
      "metadata": {
        "id": "BLiZwg4GrCIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_train_50, loss_val_50, y_train_50 = train_model(model, train_loader_50, val_loader_50,4)"
      ],
      "metadata": {
        "id": "8nO7c8MDrXHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediciton_50 - prediction_printer(test_loader_50, model)"
      ],
      "metadata": {
        "id": "rW5tFOeRrf4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_data_20 = data_prep_function(data, window_size = 20)"
      ],
      "metadata": {
        "id": "8YMxomEoKXB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formated_data_20 = matrix_formatting_and_normalize_dataset(window_data_20)"
      ],
      "metadata": {
        "id": "6Yd7fnKbKZNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_20, val_loader_20, test_loader_20 = feature_target_modeling(formated_data_20)"
      ],
      "metadata": {
        "id": "tyeAAIQzKZX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_train_20, loss_val_20, y_train_20 = train_model(model, train_loader_20, val_loader_20,4)"
      ],
      "metadata": {
        "id": "HqBiu0mIKZgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_20 = prediction_printer(test_loader_20, model)"
      ],
      "metadata": {
        "id": "IUl0I1iSKZmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RgzsF5XIK80H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uNOFU1GBKZq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q2QKu2EqKZul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-mYi-k6KZx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-kNRAh_BKZ1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f6WW2tjBKZ3s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}