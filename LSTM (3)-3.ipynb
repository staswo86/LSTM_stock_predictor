{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aSDFGNM,HJK,IO;PKUJHGRAZXsdsgfbyctu6579###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "baxAlMuODzzu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def read_data_function(file_name):\n",
    "    data_frame = pd.read_csv(file_name, sep = \",\")\n",
    "    data_frame = data_frame[[\"date\", \"open\"]]\n",
    "    data_frame[\"date\"] = pd.to_datetime(data_frame[\"date\"], format = \"%Y-%m-%d\")\n",
    "    data_frame.index = data_frame.pop(\"date\")\n",
    "    #scaler = MinMaxScaler(feature_range = (-1, 1))\n",
    "    #stock_price_val = data_frame[\"open\"].values.reshape(-1,1)\n",
    "    #data_frame[\"open\"] = scaler.fit_transform(stock_price_val)\n",
    "\n",
    "\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "EN7bdkTsEJKq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def data_prep_function(sequence_dataset,window_size):\n",
    "    #sequence_dataset = [np.array(sequence_dataset[i*sliding_window_size: (i + 1) * sliding_window_size]) for i in range(len(sequence_dataset) // sliding_window_size)]\n",
    "    #X = np.array([sequenc_dataset[i: i + num_steps] for i in range(len(sequence_dataset) - sliding_window_size)])\n",
    "    #y = np.array([sequence_dataset[i + num_steps] for i in range(len(sequence_dataset) - sliding_window_size)])\n",
    "    for i in range(1, window_size + 1):\n",
    "        sequence_dataset[f\"open-{i}\"] = sequence_dataset[\"open\"].shift(i)\n",
    "\n",
    "\n",
    "    sequence_dataset.dropna(inplace = True)\n",
    "\n",
    "\n",
    "    return sequence_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "xUi3qzNmEOF7"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def matrix_formatting_and_normalize_dataset(sliding_window_dataset):\n",
    "    sliding_window_dataset = sliding_window_dataset.to_numpy()\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(sliding_window_dataset)\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "b-PdYgcqGDdK"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch\n",
    "def feature_target_modeling(dataset):\n",
    "    X = dataset[:, 1:]\n",
    "    y = dataset[:,0]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "    y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
    "    X_train_tensor = torch.tensor(X_train, dtype = torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype = torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype = torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype = torch.float32)\n",
    "    \n",
    "    return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor\n",
    "\n",
    "\n",
    "def split_groups(X_train, X_test, y_train, y_test):\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_sub, val_sub = random_split(train_dataset, [train_size, val_size])\n",
    "    val_loader = torch.utils.data.DataLoader(val_sub, batch_size=16, shuffle=False)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 16, shuffle = True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 16, shuffle = False)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "tP3T7yv7GKJn"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_sz, hidden_sz, output_sz = 1):\n",
    "        super().__init__()\n",
    "        self.input_sz= input_sz\n",
    "        self.hidden_size= hidden_sz\n",
    "        self.W= nn.Parameter(torch.Tensor(input_sz, hidden_sz* 4))\n",
    "        self.U= nn.Parameter(torch.Tensor(hidden_sz, hidden_sz* 4))\n",
    "        self.bias= nn.Parameter(torch.Tensor(hidden_sz* 4))\n",
    "        self.linear = nn.Linear(hidden_sz, output_sz)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        stdv= 1.0/ math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x,\n",
    "                init_states=None):\n",
    "\n",
    "\n",
    "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
    "        bs, seq_sz, _= x.size()\n",
    "        hidden_seq= []\n",
    "        if init_states is None:\n",
    "\n",
    "            h_t, c_t= (torch.zeros(bs, self.hidden_size).to(x.device),\n",
    "                            torch.zeros(bs, self.hidden_size).to(x.device))\n",
    "        else:\n",
    "\n",
    "             h_t, c_t= init_states\n",
    "\n",
    "        HS= self.hidden_size\n",
    "        for t in range(seq_sz):\n",
    "                        x_t= x[:, t, :]\n",
    "            # batch the computations into a single matrix multiplication\n",
    "                        gates= x_t@ self.W+ h_t@ self.U+ self.bias\n",
    "                        i_t, f_t, g_t, o_t= (\n",
    "                            torch.sigmoid(gates[:, :HS]),# input\n",
    "                            torch.sigmoid(gates[:, HS:HS*2]),# forget\n",
    "                            torch.tanh(gates[:, HS*2:HS*3]),\n",
    "                            torch.sigmoid(gates[:, HS*3:]),# output\n",
    "                        )\n",
    "                        c_t= f_t* c_t+ i_t* g_t\n",
    "                        h_t= o_t* torch.tanh(c_t)\n",
    "                        hidden_seq.append(h_t.unsqueeze(0))\n",
    "        hidden_seq= torch.cat(hidden_seq, dim=0)\n",
    "            # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
    "        hidden_seq= hidden_seq.transpose(0, 1).contiguous()\n",
    "        output = self.linear(hidden_seq[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "VI9VCxH2GOdz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def train_model(model: torch.nn.Module, train_dataloader : DataLoader,val_dataloader: DataLoader, epochs: int):\n",
    "    model.train(True)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    loss_vectorize_train = np.zeros(epochs)\n",
    "    loss_vectorize_val = np.zeros(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss_train = 0\n",
    "        for x_batch, y_batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x_batch)\n",
    "            loss = loss_function(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss_train += loss.item()\n",
    "            loss_vectorize_train[epoch] = epoch_loss_train / len(train_dataloader)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            epoch_loss_val = 0\n",
    "            for x_batch_val, y_batch_val in val_dataloader:\n",
    "                preds_2 = model(x_batch_val)\n",
    "                loss = loss_function(preds_2, y_batch_val)\n",
    "                epoch_loss_val += loss.item()\n",
    "                loss_vectorize_val[epoch] = epoch_loss_val / len(val_dataloader)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train loss {loss_vectorize_train[epoch] : .6f} || Val loss {loss_vectorize_val[epoch] : .6f}\")\n",
    "\n",
    "\n",
    "\n",
    "    return loss_vectorize_train, loss_vectorize_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ojDPc76RaaMA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "def prediction_printer(test_loader : DataLoader, model):\n",
    "  with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "      prediction_test = model(x_batch).numpy()\n",
    "\n",
    "  return prediction_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "nkGp-kBWGSld",
    "outputId": "9d7328de-6d13-466d-ac23-405da85b83a9"
   },
   "outputs": [],
   "source": [
    "data = read_data_function(\"all_stocks_5yr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "LOdHkg1xIdOV"
   },
   "outputs": [],
   "source": [
    "window_data_10 = data_prep_function(data, window_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyCrhWM0uSq5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "Mnovcug4Idv6"
   },
   "outputs": [],
   "source": [
    "formated_data_10 = matrix_formatting_and_normalize_dataset(window_data_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_10, X_test_10, y_train_10, y_test_10 = feature_target_modeling(formated_data_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "He3sHR_8Id7Y"
   },
   "outputs": [],
   "source": [
    "train_loader_10, val_loader_10, test_loader_10 = split_groups(X_train_10, X_test_10, y_train_10, y_test_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "Ae7_rZj5IeFh"
   },
   "outputs": [],
   "source": [
    "model = CustomLSTM(1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "SRlsP0OcNdO4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss  0.000010 || Val loss  0.000007\n",
      "Epoch 1: Train loss  0.000009 || Val loss  0.000005\n",
      "Epoch 2: Train loss  0.000009 || Val loss  0.000005\n",
      "Epoch 3: Train loss  0.000009 || Val loss  0.000004\n"
     ]
    }
   ],
   "source": [
    "loss_train_10, loss_val_10 = train_model(model, train_loader_10, val_loader_10,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "ntJ5rZ92J6OH"
   },
   "outputs": [],
   "source": [
    "predictions_10 = prediction_printer(test_loader_10, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "aNJzOzXdIeXO"
   },
   "outputs": [],
   "source": [
    "window_data_50 = data_prep_function(data, window_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "formated_data_50 = matrix_formatting_and_normalize_dataset(window_data_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_50, X_test_50, y_train_50, y_test_50 = feature_target_modeling(formated_data_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "mEM5voyIrRJA"
   },
   "outputs": [],
   "source": [
    "train_loader_50, val_loader_50, test_loader_50 = split_groups(X_train_50, X_test_50, y_train_50, y_test_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "8nO7c8MDrXHh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss  0.000025 || Val loss  0.000009\n",
      "Epoch 1: Train loss  0.000012 || Val loss  0.000010\n",
      "Epoch 2: Train loss  0.000011 || Val loss  0.000013\n",
      "Epoch 3: Train loss  0.000011 || Val loss  0.000011\n"
     ]
    }
   ],
   "source": [
    "loss_train_50, loss_val_50 = train_model(model, train_loader_50, val_loader_50,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "rW5tFOeRrf4r"
   },
   "outputs": [],
   "source": [
    "prediciton_50 = prediction_printer(test_loader_50, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "8YMxomEoKXB1"
   },
   "outputs": [],
   "source": [
    "window_data_20 = data_prep_function(data, window_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "6Yd7fnKbKZNt"
   },
   "outputs": [],
   "source": [
    "formated_data_20 = matrix_formatting_and_normalize_dataset(window_data_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_20, X_test_20, y_train_20, y_test_20 = feature_target_modeling(formated_data_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "tyeAAIQzKZX6"
   },
   "outputs": [],
   "source": [
    "train_loader_20, val_loader_20, test_loader_20 = split_groups(X_train_20, X_test_20, y_train_20, y_test_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "HqBiu0mIKZgw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss  0.000009 || Val loss  0.000010\n",
      "Epoch 1: Train loss  0.000009 || Val loss  0.000011\n",
      "Epoch 2: Train loss  0.000009 || Val loss  0.000011\n",
      "Epoch 3: Train loss  0.000009 || Val loss  0.000010\n"
     ]
    }
   ],
   "source": [
    "loss_train_20, loss_val_20,  = train_model(model, train_loader_20, val_loader_20,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "IUl0I1iSKZmg"
   },
   "outputs": [],
   "source": [
    "prediction_20 = prediction_printer(test_loader_20, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "RgzsF5XIK80H"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_1(y_train, predictions):\n",
    "    plt.plot(y_train, label = \"Actual Open\")\n",
    "    plt.plot(predictions, laebl = \"Predicted Close\")\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_train(prediction,window_size, X_train):\n",
    "    prediction_train = prediction.flatten()\n",
    "\n",
    "    X_train_intial = np.zeros((X_train.shape[0], window_size + 1))\n",
    "    X_train_initial[:,0] = prediction_train\n",
    "    X_train_initial = scaler.inverse_transform(X_train_initial)\n",
    "    data = read_data_function(\"all_stocks_5yr.csv\")\n",
    "    prediction_train_x = data(X_train_initial[:,0])\n",
    "    \n",
    "\n",
    "    y_train_initial = np.zeros((X_train.shape[0], window_size + 1))\n",
    "    y_train_initial[:, 0] = y_train.flatten()\n",
    "    y_train_initial = scaler.inverse_transorm(y_train_initial)\n",
    "    y_train_initial_final = data(y_train_initial[:,0])\n",
    "\n",
    "\n",
    "\n",
    "    plt.plot(y_train_initial_final, label = \"Actual Open\")\n",
    "    plt.plot(prediction_train_x, laebl = \"Predicted Close\")\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"open\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_test(model, window_size, X_test):\n",
    "    prediction_test_x = model(X_test).numpy().flatten()\n",
    "    X_test_intial = np.zeros((X_test.shape[0], windohgffgfff_size + 1))\n",
    "    X_test_initial[:,0] = prediction_test_x\n",
    "    X_test_initial = scaler.inverse_transform(X_test_initial)\n",
    "    data = read_data_function(\"all_stocks_5yr.csv\")\n",
    "    prediction_test_x = data(X_test_initial[:,0])\n",
    "\n",
    "\n",
    "\n",
    "    y_test_initial = np.zeros((X_test.shape[0], window_size + 1))\n",
    "    y_test_initial[:, 0] = y_test.flatten()\n",
    "    y_test_initial = scaler.inverse_transorm(y_test_initial)\n",
    "    y_test_final = data(y_test_initial[:,0])\n",
    "\n",
    "\n",
    "\n",
    "    plt.plot(y_test_final, label = \"Actual Open\")\n",
    "    plt.plot(prediction_test_x, laebl = \"Predicted Close\")\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"open\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNOFU1GBKZq7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2QKu2EqKZul"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-mYi-k6KZx9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-kNRAh_BKZ1W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6WW2tjBKZ3s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
